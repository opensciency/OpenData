# Lesson 7: Finding and Using Open Data

**Lead**: Jannatul

**Contributors**: Hugh, Douglas, Andrea, Esther

---

```{contents}
:local:
```


## Introduction:

Since Open Science is a data-intensive endeavor, sharing, finding and using Open Data in a proper way is the ultimate goal of this module, including previous lessons on the benefits of Open Data, responsible Open Data, FAIR and CARE principles, planning for, and sharing Open Data. However, the quantity of information was much more manageable just a decade ago. With the pace of advanced digital technologies, today’s researchers need to be informed about the latest publications and utilize the existing data to;



* apply and broaden the research hypotheses',
* validate their findings against references and benchmark data,
* train their AI (Artificial Intelligence) and ML (Machine Learning) models
* reduce costs associated with producing new data
* and practice various research activities by minimizing time and energy consuming tasks related to Open Data reuse and reproducibility.

While adhering to the FARE and CARE principles, the data user (e.g., policy-maker, developer, decision maker, researcher, citizen) can open the room for collaborations and optimize the reuse of open data.

This lesson will give an overview of possible ways and guidelines of finding and re-using Open Data to support communities’ efforts as we try to mobilize the Open Science movement forward towards a technologically driven world.


### 7.1 How to find Open data


The reusability of openly shared data relies on the prospects of it being found in the first place, therefore data findability is a key step in accessing and utilizing data. There are three major ways to find Open Data that are shared by researchers – repository, web search, and literature search.

#### Repositories

Ideally, Open Data should be available in repositories where the datasets are properly indexed and assigned a unique persistent identifier (as discussed in **Lesson 6 – Sharing Open Data**) thereby ensuring the data is unambiguously identifiable, searchable, discoverable along with associated metadata and documentations.

Therefore, the first step in finding Open Data related to your field is to identify discipline specific repositories (if there are any) and search for datasets there (see **Lesson 6.4 – Repositories and Other Sharing Methods**).

 Find repositories in your field:



* _[Re3data.org](http://re3data.org) is a global registry of research data repositories that covers research data repositories from different academic disciplines._
* _[FAIRsharing](https://fairsharing.org/) is a curated, informative, and educational resource on data and metadata standards, inter-related to databases and data policies._
* _Recommended repositories by publishers (e.g., Recommended Data Repositories suggested by [Scientific Data](https://www.nature.com/sdata/policies/repositories#envgeo) and[ PLOS One](https://journals.plos.org/plosone/s/recommended-repositories))_
* _[World Data System](https://www.worlddatasystem.org/) represents a network of repositories._

_Examples of generic repositories:_



* _[Zenodo](https://zenodo.org/)_
* _[Mendeley Data](https://data.mendeley.com/)_
* _[Figshare](https://figshare.com/)_
* _[Dryad](https://datadryad.org/stash)_

The[ Generalist Repository Comparison Chart](https://zenodo.org/record/3946720#.YUKQ18RS-Uk) is a tool you can use to decide where to store and share their FAIR data outside of their institutional repositories. Dataverse has also published a[ comparative review of eight data repositories.](https://dataverse.org/blog/comparative-review-various-data-repositories)


#### Web-searches

To explore a wide variety of datasets from projects or popular topics, the use of a more general search engine can be helpful. Some disciplines or large institutions such as NASA and the National Institute of Health’s National Center for Biotechnology Information (NCBI) offer their own portal where you can search for their datasets, related publications and oftentimes tools for analysis (e.g., EMBL's European Bioinformatics Institute[ https://www.ebi.ac.uk/](https://www.ebi.ac.uk/) ). There are also an increasing number of international and national data portals to enable data discoveries.



**Generic data search portals:**



* Google[ https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)
* Kaggle[ https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
* Wikidata[ https://www.wikidata.org/wiki/Wikidata:Main_Page](https://www.wikidata.org/wiki/Wikidata:Main_Page)
* Open Data Network [https://www.opendatanetwork.com/](https://www.opendatanetwork.com/)
* Awesome Public Datasets[ https://github.com/awesomedata/awesome-public-datasets#readme](https://github.com/awesomedata/awesome-public-datasets#readme)

**Examples of Discipline specific:**



* NASA Earth[ https://www.earthdata.nasa.gov/](https://www.earthdata.nasa.gov/)
* Cern[ https://opendata.cern.ch/](https://opendata.cern.ch/)
* NCBI National Center for Biotechnology Information[ https://www.ncbi.nlm.nih.gov/](https://www.ncbi.nlm.nih.gov/)
* EMBL's European Bioinformatics Institute[ https://www.ebi.ac.uk/](https://www.ebi.ac.uk/)
* ISPCR[ https://www.icpsr.umich.edu/web/pages/](https://www.icpsr.umich.edu/web/pages/)
* International Monetary Fund  [https://www.imf.org/en/Data](https://www.imf.org/en/Data)
* NOAA Climate Data Online [https://www.ncdc.noaa.gov/cdo-web/datasets](https://www.ncdc.noaa.gov/cdo-web/datasets)  
* Federal Reserve Economic Research [https://fred.stlouisfed.org/](https://fred.stlouisfed.org/)
* USGS EarthExplorer [https://earthexplorer.usgs.gov/](https://earthexplorer.usgs.gov/)
* Open Science Data Cloud (OSDC) [https://www.opensciencedatacloud.org/](https://www.opensciencedatacloud.org/)
* NASA Planetary Data System [https://pds.nasa.gov/](https://pds.nasa.gov/)


#### **Examples of National or international data portal**



* US Federal data[ https://data.gov/](https://data.gov/)
* EU Data Portal[ https://data.europa.eu/en](https://data.europa.eu/en)
* WHO[ https://apps.who.int/gho/data/node.home](https://apps.who.int/gho/data/node.home)
* THE WORLD BANK [https://data.worldbank.org/](https://data.worldbank.org/)
* DATA.GOV.UK [https://www.data.gov.uk/](https://www.data.gov.uk/)
* UNICEF [https://data.unicef.org/](https://data.unicef.org/)


#### Literature search

While not ideal, datasets are often attached to scholarly publications in the form of supplementary material, or referenced in text where to find them e.g. GitHub repository or personal/institutional websites. In addition, there are emerging journals and special collections/issues focused on describing and publishing data (e.g. Nucleic Acids Research database issues[ https://doi.org/10.1093/nar/gkab1195](https://doi.org/10.1093/nar/gkab1195), Scientific Data, Earth System Science Data, etc.). In other words, while the datasets are openly available in these media, they are not properly indexed and therefore not very findable nor machine readable.

Finding academic publications can be a challenge in itself depending on the discipline and field of study. For instance, in life science and biomedical research, there are a number of repositories and search engines (e.g. PubMed, EuropePMC) indexing research outputs (e.g. publications, abstracts, references and communications) from various journals.

However in other disciplines (e.g. arts and humanities), search is often carried out with general search engines or research databases such as Google Scholar and JSTOR. In that case, it is advisable to reach out to library personnel and community members for further advice on where to find related literature and data, see lesson 5.4 Help section.

**Generic:**



* Google Scholar[ https://scholar.google.com](https://scholar.google.com)
* Open knowledge map: A visual interface allowing the exploration of interconnected topics with relevant documents and concepts.  [https://openknowledgemaps.org/](https://openknowledgemaps.org/)
* JSTOR a wide range of scholarly content[ https://www.jstor.org/](https://www.jstor.org/)
* ResearchGate[ https://www.researchgate.net/search](https://www.researchgate.net/search)

**Discipline specific:**



* EuropePMC Life sciences [https://europepmc.org/](https://europepmc.org/)
* Pubmed biomedical literature [https://pubmed.ncbi.nlm.nih.gov/](https://pubmed.ncbi.nlm.nih.gov/)
* arXiv is a free distribution service and an open-access archive for scholarly pre-prints in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics [https://arxiv.org/](https://arxiv.org/)
* Biorxiv Preprint server for biology [https://www.biorxiv.org/](https://www.biorxiv.org/)
* EarthArXiv ([https://eartharxiv.org](https://eartharxiv.org)) and Earth and Space Science Open Archive ([https://essoar.org](https://essoar.org))
* ASAPbio provides a catalog of preprint servers [https://asapbio.org/preprint-servers](https://asapbio.org/preprint-servers)  


### 7.2 What are you allowed to do with it?

Once a dataset is found, the degree to which it can be reused may be limited in various ways. This can include the introduction of partial or total restrictions to address safety concerns, legal rights, ethical principles, protection of community interests and well being (see lesson 4 CARE and FAIR principles), and commercial interests. A well documented dataset will include details on what you are allowed and not allowed to do with the data, and this information is typically outlined in the documents below.


#### Data Access Statement

Data Access statements, (described in lesson 6.5) outline the restrictions setup on specific datasets as well as what the users need to provide or abide by in order to gain access to the data. At times you may need to submit a Data Access Request specifying what you intend to do with this data and how you intend to fulfill the requirements as set by the data owner.


#### Licenses

It is imperative that one understands the license a dataset is released under before data reuse. Without a good understanding of what a license allows, it will open researchers up to copyright infringement or other intellectual property issues. The purpose of licenses is described in section 6 in more detail.

Some dataset licenses are wholly open (see various flavors of the Creative Commons License - [https://creativecommons.org/](https://creativecommons.org/), the British Open Government License- [https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/](https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/),  and the World Bank Terms of Use - [https://www.worldbank.org/en/about/legal/terms-and-conditions](https://www.worldbank.org/en/about/legal/terms-and-conditions) - among others), while others have restrictions and requirements for usage. Specifically, there are often requirements for attribution and the license derived products must apply to themselves.

More information on licenses, and copyright can be found at the DCC ([https://www.dcc.ac.uk/guidance/how-guides/license-research-data#x1-13000doc](https://www.dcc.ac.uk/guidance/how-guides/license-research-data#x1-13000doc)) and The Turing Way ([https://the-turing-way.netlify.app/reproducible-research/licensing/licensing-data.html](https://the-turing-way.netlify.app/reproducible-research/licensing/licensing-data.html)).


##### Citations

As noted in 6.3 it is important to cite the data that you use appropriately. This ensures that other researchers can easily find the data set you have used and that the developers of the data set get credit for their work.


### 7.3 How can you use it?

Data reuse is contingent on a number of factors including quality of data, access and reuse conditions, data findability, and more. Below we outline a few essential elements that enable you to assess the relevance and quality of datasets .


#### Documentation

Dataset discovery relies on the quality of documentation associated with the datasets, i.e. metadata.  Search engines rely on key information about the datasets which allows them to be indexed and searched,  more on this [https://www.blog.google/products/search/making-it-easier-discover-datasets/](https://www.blog.google/products/search/making-it-easier-discover-datasets/)

No matter how comprehensive the data is, a general rule of thumb is that you should be able to understand what the dataset is about, what is the stage of the data, and what to expect before downloading and processing it.

Some core descriptors to look for are:



* Who created this data?
* When was it published?
* How was the data collected?
* What are the tools used for processing and handling the data?
* What are the terms for using it?
* What are the disciplines that used the data?
* What are the communities that have developed around it?

Ideally a fully documented Data Reuse plan should accompany the datasets providing detailed information on What, Who, Where, When and How, see  Data Reuse Planning Template by mozilla open science

[https://github.com/mozillascience/working-open-workshop/blob/8931bf19bedb99166b931f82dbd2fba815dc6fe1/handouts/data_reuse_plan_template.md](https://github.com/mozillascience/working-open-workshop/blob/8931bf19bedb99166b931f82dbd2fba815dc6fe1/handouts/data_reuse_plan_template.md)


#### Formats

As noted previously, the format of data is key for its interoperability. Data formats are in two classes. Closed, proprietary,  formats are those where the specification for the way the data is stored is not publicly available and hence is tied explicitly to the software (or hardware) developed by an organization. Open formats have a specification that is available to all (though some may be proprietary and hence subject to conditions that will vary on the license terms applied to them).  Open data formats are cataloged in FAIRsharing.org.

The format of a file should be described in the metadata associated with the data set. If metadata is not available then inspecting the file name (i.e. the postfix to the file name that comes after the period) can give an indication of what the file type is. Opening the file with software for a specific data format provides a similar test.

If the format isn’t clear or is using a closed format where there isn’t software to access the data it is worth contacting the authors of the data set to see if they can help though the authors are not obliged to reply to you. Reaching out to the communities outlined in lesson 5 can also provide advice.


#### Tools

Many of the details here are discussed in the Open Tools lesson. In the first instance in order to ensure that data is being read properly it is useful to check if a previously published analysis of the data can be reproduced. Papers outlining the analysis  should provide some description of the software used and the necessary steps in the analysis. In reproducing such an analysis one can determine if proprietary software is necessary to use the data and if you have all the necessary software or indeed source code to reproduce the results. If the results can be reproduced then you can be more confident in reusing the data. As described above, contacting the authors if there is an issue or likewise contacting communities described in lesson 5 can also help here.  


#### Data Cleaning

Data cleaning refers to the preliminary step where the quality of data is assessed and any errors, inconsistencies or formatting issues are detected and resolved prior to using the datasets. It also includes making decisions on what to do with missing or incomplete data.

Each research community, either implicitly or explicitly, has its own pipeline for cleaning data, for example how to deal with missing values, normalize data and  so on. Likewise it is important to apply community-based measures of quality. It is therefore advisable to consult with data managers, librarians and other community members for common practices see lesson 5.4 Help.

Data cleaning is important to provide access to accurate and consistent data, this is often highlighted in the phrase Garbage in-Garbage out, meaning that flawed data (e.g. duplicated or missing information) will produce incorrect or misleading outputs (e.g. statistics). Data quality can be reduced in several ways, including misspellings or typos during data entry, missing values, mixed formats, replicated entries of the same real-world entity or other invalid data

This process requires significant human intervention (including in some cases sectoral or community efforts), which may consume time and energy. Anecdotally, preparing data in this fashion can take up to 80% of the time involved in the analysis of data.

 (Hellerstein, J. M. (2008). Quantitative data cleaning for large databases. United Nations Economic Commission for Europe (UNECE), 25, 1-42. [https://dataresponsibly.github.io/courses/documents/Hellerstein_2008.pdf](https://dataresponsibly.github.io/courses/documents/Hellerstein_2008.pdf)), (Chu, X., Ilyas, I. F., Krishnan, S., & Wang, J. (2016, June). Data cleaning: Overview and emerging challenges. In _Proceedings of the 2016 international conference onmanagement of data_ (pp. 2201-2206. [https://doi.org/10.1145/2882903.2912574](https://doi.org/10.1145/2882903.2912574)).

**Guides: **



* Harvard- Analysis Ready Datasets [https://datamanagement.hms.harvard.edu/analyze/analysis-ready-datasets](https://datamanagement.hms.harvard.edu/analyze/analysis-ready-datasets)
* [https://en.wikipedia.org/wiki/Data_cleansing#Data_quality](https://en.wikipedia.org/wiki/Data_cleansing#Data_quality)
* [https://www.kaggle.com/learn/data-cleaning](https://www.kaggle.com/learn/data-cleaning)
* [https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4) _ _
* Carpentries [https://datacarpentry.org/lessons/](https://datacarpentry.org/lessons/)

**Tools:  **



* [https://openrefine.org/](https://openrefine.org/)
* See more in tools module


## Conclusion:

Over the present decade, there has been a surge of interest from both industry and academia in Open Data, not only for record keeping but also to support a variety of decision-making tasks. Creating new data is costly and sometimes redundant. Therefore there is an increased emphasis on reuse of Open Data. Below we outline a handy checklist that outlines what to look for.  


### Checklist:
1. Find a dataset
2. Check if you can access it (DAS)
3. Check license: Is it fully open or partly open?
4. What are you allowed to use it for?
5. Documentation:
    1. Is it raw or processed data?
    2. Can you understand what this dataset is about?
    3. Is the dataset missing parts of the data?
    4. Which units of measurements are used?
    5. What quality controls were in place?
6. What file formats is the data in?
7. Can you open it and use it with available software?
8. Is all necessary software or source code included and made available?
9. Do you understand the provenance of the data? I.e. Can you identify who owns this data?
10. Can you contact the original authors?
11. How can you cite the data?


## Assessment



* Can you find a dataset related to your field?
* What kind of license does it have?
* What does the license allow you to do?    


## References
